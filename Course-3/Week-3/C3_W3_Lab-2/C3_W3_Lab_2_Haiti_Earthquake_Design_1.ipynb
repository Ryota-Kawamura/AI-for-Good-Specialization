{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Haiti 2 - Cleaning and Processing Haiti Data</h1>\n",
    "\n",
    "In the last notebook you explored the dataset. In this notebook, you will load in the same [text dataset](https://github.com/rmunro/disaster_response_messages). You'll then clean and process the data using several Natural Language Processing (NLP) techniques.\n",
    "\n",
    "**In this lab you will apply the following steps:**\n",
    "1. Import Python Packages\n",
    "2. Load the data\n",
    "3. Text cleaning and processing\n",
    "4. Explore the number of tokens\n",
    "5. Represent a word as a count-based vector (Bag of Words)\n",
    "6. Explore the top words\n",
    "\n",
    "**Note:** Keep in mind here that the processing techniques you'll use on text messages in your dataset is only possible because you have the English translation of the messages in hand. If you only had the messages in their original Haitian Kreyol or some other language, your text processing and modeling techniques may need to be quite different depending on the nuances of the language, grammar, and available tools for work in that language. For this reason, it's worth noting again that one of the most impactful things you can work on for low-resources languages are the general purpose tools for things like translation, search, mapping, or information extraction, rather than custom machine learning solutions for particular scenarios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from termcolor import colored\n",
    "import gensim\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import utils\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "print('All packages imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded successfully!\n",
      "Haiti data selected!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>split</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>PII</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>...</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "      <th>event</th>\n",
       "      <th>actionable_haiti</th>\n",
       "      <th>date_haiti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>haiti_earthquake</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-02-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>train</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>haiti_earthquake</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>train</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>haiti_earthquake</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>train</td>\n",
       "      <td>Information about the National Palace-</td>\n",
       "      <td>Informtion au nivaux palais nationl</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>haiti_earthquake</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Storm at sacred heart of jesus</td>\n",
       "      <td>Cyclone Coeur sacr de jesus</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>haiti_earthquake</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  split                                            message  \\\n",
       "0   2  train  Weather update - a cold front from Cuba that c...   \n",
       "1   7  train            Is the Hurricane over or is it not over   \n",
       "2  12  train  says: west side of Haiti, rest of the country ...   \n",
       "3  14  train             Information about the National Palace-   \n",
       "4  15  train                     Storm at sacred heart of jesus   \n",
       "\n",
       "                                            original   genre  related  PII  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1    0   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1    0   \n",
       "2  facade ouest d Haiti et le reste du pays aujou...  direct        1    0   \n",
       "3                Informtion au nivaux palais nationl  direct        0    0   \n",
       "4                        Cyclone Coeur sacr de jesus  direct        1    0   \n",
       "\n",
       "   request  offer  aid_related  ...  floods  storm  fire  earthquake  cold  \\\n",
       "0        0      0            0  ...       0      0     0           0     0   \n",
       "1        0      0            1  ...       0      1     0           0     0   \n",
       "2        0      0            0  ...       0      0     0           0     0   \n",
       "3        0      0            0  ...       0      0     0           0     0   \n",
       "4        0      0            0  ...       0      1     0           0     0   \n",
       "\n",
       "   other_weather  direct_report             event  actionable_haiti  \\\n",
       "0              0              0  haiti_earthquake                 0   \n",
       "1              0              0  haiti_earthquake                 0   \n",
       "2              0              0  haiti_earthquake                 0   \n",
       "3              0              0  haiti_earthquake                 0   \n",
       "4              0              0  haiti_earthquake                 0   \n",
       "\n",
       "   date_haiti  \n",
       "0  2010-02-09  \n",
       "1  2010-01-17  \n",
       "2  2010-01-17  \n",
       "3  2010-01-17  \n",
       "4  2010-01-17  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the datasets\n",
    "training_data = pd.read_csv(\"data/disaster_response_training.csv\", low_memory=False)\n",
    "validation_data = pd.read_csv(\"data/disaster_response_validation.csv\", low_memory=False)\n",
    "test_data = pd.read_csv(\"data/disaster_response_test.csv\", low_memory=False)\n",
    "print('Datasets loaded successfully!')\n",
    "\n",
    "# Merge the three datasets\n",
    "full_data = training_data.append(validation_data).append(test_data)\n",
    "# Fix column data type\n",
    "full_data['original'] = full_data['original'].astype(str)\n",
    "\n",
    "# Select only the Haiti data\n",
    "haiti_df = full_data[full_data.event == 'haiti_earthquake']\n",
    "# Fix column data type\n",
    "haiti_df['actionable_haiti'] = haiti_df.actionable_haiti.astype('int64')\n",
    "haiti_df['date_haiti'] = pd.to_datetime(haiti_df.date_haiti)\n",
    "print('Haiti data selected!')\n",
    "haiti_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text cleaning and processing\n",
    "\n",
    "Before you can perform any modelling on the text, you need to go through a series of steps to clean and process the data. The steps include\n",
    "\n",
    "1. **Tokenize:** This splits strings based on white spaces and punctuation. Further, it expands contractions (i.e `can't` becomes `ca` and `n't`\n",
    "2. **Remove puncutation:** To remove the puncuations from words, you will use `string.punctuation` which is a list of all punctuation symbols. You'll notice that the list is not exhaustive and you may need to add additional punctuations based on your specific dataset (check the `utils` file for details).\n",
    "3. **Standardize letter case:** You also want to ensure that all words follow the same format in order to recognize duplicate words in a message. To do this, you'll convert all words to lowercase. \n",
    "4. **Remove stop words:** These are common words that are often used in speech or text that you may not want to include in our final analysis, as words like \"and\" are very common and may skew the results of what you're trying to analyze.\n",
    "5. **Lemmatize each word:** In our case, you want to know what general topics are being spoken about in relation to the Haiti earthquake, and so it doesn't matter as much to us whether someone mentions \"help\", \"helped\", or \"helping\". Lemmatization is not the best solution for every kind of task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Clean and process a single message\n",
    "Before you process the entire dataset, let's look at a random message and see how each step cleans and processes each word in the message. In the below cell you see the code with each of the steps. Then in the next cell you can run it in an interactive mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stopwords and punctuation from the utils file\n",
    "STOP_WORDS = utils.STOP_WORDS\n",
    "punctuation = utils.punctuation\n",
    "# Instantiate the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def process_random_message(df):\n",
    "    message = utils.get_random_message(df)\n",
    "    \n",
    "    # Step 1: Tokenize and get POS tags\n",
    "    tokens = nltk.pos_tag(word_tokenize(message))\n",
    "    print(colored(\"Step 1: Tokenize\\n\", \"blue\"), \"{}\\n\".format([w[0] for w in tokens]))\n",
    "    \n",
    "    # Step 2: Standardize Lettercase\n",
    "    tokens = [(w[0].lower(), w[1]) for w in tokens]\n",
    "    print(colored(\"Step 2: Lowercase\\n\", \"blue\"), \"{}\\n\".format([w[0] for w in tokens]))\n",
    "\n",
    "    # Step 3: Remove Puncuation\n",
    "    tokens = [w for w in tokens if w[0] not in punctuation]\n",
    "    print(colored(\"Step 3: Remove punctuation\\n\", \"blue\"), \"{}\\n\".format([w[0] for w in tokens]))\n",
    "\n",
    "    # Step 4: Remove stop words\n",
    "    tokens = [w for w in tokens if w[0] not in STOP_WORDS]\n",
    "    print(colored(\"Step 5: Remove stop words\\n\", \"blue\"), \"{}\\n\".format([w[0] for w in tokens]))\n",
    "\n",
    "    # Step 5: Lemmatize each word \n",
    "    tokens = [lemmatizer.lemmatize(w[0], utils.pos_tag_convert(w[1])) for w in tokens]\n",
    "    print(colored(\"Step 4: Lemmatize\\n\", \"blue\"), \"{}\\n\".format(tokens))\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8701d75776488582458eefcb0c138c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Data to show', options=('All', 'medical_help', 'medical_products',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.interact_with_filters(process_random_message, haiti_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 3.2 Process the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can put all of the following cleaning together into one method (defined as `clean_tokenize_process_text` in the `utils` file). Now you can run it for the whole dataframe and save the tokenized messages to a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>split</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>PII</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>...</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "      <th>event</th>\n",
       "      <th>actionable_haiti</th>\n",
       "      <th>date_haiti</th>\n",
       "      <th>message_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>haiti_earthquake</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-02-09</td>\n",
       "      <td>[weather, update, cold, front, cuba, could, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>train</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>haiti_earthquake</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-17</td>\n",
       "      <td>[hurricane]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>train</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>haiti_earthquake</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-17</td>\n",
       "      <td>[say, west, side, haiti, rest, country, today,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>train</td>\n",
       "      <td>Information about the National Palace-</td>\n",
       "      <td>Informtion au nivaux palais nationl</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>haiti_earthquake</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-17</td>\n",
       "      <td>[information, national, palace-]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Storm at sacred heart of jesus</td>\n",
       "      <td>Cyclone Coeur sacr de jesus</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>haiti_earthquake</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-17</td>\n",
       "      <td>[storm, sacred, heart, jesus]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  split                                            message  \\\n",
       "0   2  train  Weather update - a cold front from Cuba that c...   \n",
       "1   7  train            Is the Hurricane over or is it not over   \n",
       "2  12  train  says: west side of Haiti, rest of the country ...   \n",
       "3  14  train             Information about the National Palace-   \n",
       "4  15  train                     Storm at sacred heart of jesus   \n",
       "\n",
       "                                            original   genre  related  PII  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1    0   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1    0   \n",
       "2  facade ouest d Haiti et le reste du pays aujou...  direct        1    0   \n",
       "3                Informtion au nivaux palais nationl  direct        0    0   \n",
       "4                        Cyclone Coeur sacr de jesus  direct        1    0   \n",
       "\n",
       "   request  offer  aid_related  ...  storm  fire  earthquake  cold  \\\n",
       "0        0      0            0  ...      0     0           0     0   \n",
       "1        0      0            1  ...      1     0           0     0   \n",
       "2        0      0            0  ...      0     0           0     0   \n",
       "3        0      0            0  ...      0     0           0     0   \n",
       "4        0      0            0  ...      1     0           0     0   \n",
       "\n",
       "   other_weather  direct_report             event  actionable_haiti  \\\n",
       "0              0              0  haiti_earthquake                 0   \n",
       "1              0              0  haiti_earthquake                 0   \n",
       "2              0              0  haiti_earthquake                 0   \n",
       "3              0              0  haiti_earthquake                 0   \n",
       "4              0              0  haiti_earthquake                 0   \n",
       "\n",
       "   date_haiti                                     message_tokens  \n",
       "0  2010-02-09  [weather, update, cold, front, cuba, could, pa...  \n",
       "1  2010-01-17                                        [hurricane]  \n",
       "2  2010-01-17  [say, west, side, haiti, rest, country, today,...  \n",
       "3  2010-01-17                   [information, national, palace-]  \n",
       "4  2010-01-17                      [storm, sacred, heart, jesus]  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process all messages and save tokens to a new column\n",
    "haiti_df[\"message_tokens\"] = haiti_df.message.apply(\n",
    "    utils.process_text,\n",
    "    tokenizer=word_tokenize, pos_tagger=nltk.pos_tag, lemmatizer=lemmatizer, stopwords=STOP_WORDS, punctuation=punctuation\n",
    ")\n",
    "\n",
    "haiti_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now look at some of the messages and their tokens directly from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed632b773c44ab8b5c60e4df4d06da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Data to show', options=('All', 'medical_help', 'medical_products',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.interact_with_filters(utils.print_messages, haiti_df, number_of_messages=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 4. Explore the number of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Count the number of words and tokens\n",
    "What is the distribution of the length of each message? You can calculate that and use a histogram to plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a6d73323bc425faaa0774e9e8a92f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Data to show', options=('All', 'medical_help', 'medical_products',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count number of words and save as a column\n",
    "haiti_df[\"num_words\"] = haiti_df.message.apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "# Count number of tokens and save as a column\n",
    "haiti_df[\"num_tokens\"] = haiti_df.message_tokens.apply(lambda x: len(x))\n",
    "\n",
    "utils.interact_with_filters(utils.histogram_number_of_words, haiti_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Several of your messages are quite short. Let's see which messages have only 1 token after processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>message_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>Wesantyahoo.fr.Pepayisenyahoo.fr</td>\n",
       "      <td>[wesantyahoo.fr.pepayisenyahoo.fr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5474</th>\n",
       "      <td>what should do if they have earthquake</td>\n",
       "      <td>[earthquake]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7134</th>\n",
       "      <td>will we have an other earthquake</td>\n",
       "      <td>[earthquake]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7594</th>\n",
       "      <td>What can I do if there is a earthquake.</td>\n",
       "      <td>[earthquake]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7297</th>\n",
       "      <td>what do i have to do when there's earthquake?</td>\n",
       "      <td>[earthquake]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  \\\n",
       "723                Wesantyahoo.fr.Pepayisenyahoo.fr    \n",
       "5474         what should do if they have earthquake    \n",
       "7134               will we have an other earthquake    \n",
       "7594        What can I do if there is a earthquake.    \n",
       "7297  what do i have to do when there's earthquake?    \n",
       "\n",
       "                          message_tokens  \n",
       "723   [wesantyahoo.fr.pepayisenyahoo.fr]  \n",
       "5474                        [earthquake]  \n",
       "7134                        [earthquake]  \n",
       "7594                        [earthquake]  \n",
       "7297                        [earthquake]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haiti_df[haiti_df.num_tokens==1][['message', 'message_tokens']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As you can see, this helps us to find which messages may be anomalous or strange. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Represent a word as a count-based vector (Bag of Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Explore a mini corpus\n",
    "To make things easy to visualize and comprehend, you can look at a small corpus consisting of just a few messages below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f077ca983a749fa9f58a9076a1f3883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Data to show', options=('All', 'medical_help', 'medical_products',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.interact_with_filters(\n",
    "    utils.mini_corpus, haiti_df,\n",
    "    corpus_size=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Apply BoW to full corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mBefore filter: \u001b[0m Dictionary contains a total of 9310 unique words\n",
      "\u001b[34mAfter filter: \u001b[0m Dictionary contains a total of 1604 unique words\n"
     ]
    }
   ],
   "source": [
    "# All of the messages in the dataset\n",
    "corpus = haiti_df.message_tokens\n",
    "\n",
    "# Create the Dictionary\n",
    "corpus_dictionary = gensim.corpora.Dictionary(corpus)\n",
    "\n",
    "print(colored(\"Before filter: \", \"blue\"), f\"Dictionary contains a total of {len(corpus_dictionary)} unique words\")\n",
    "\n",
    "# Filter for words that occur more than 5 times overall\n",
    "corpus_dictionary.filter_extremes(no_below=5)\n",
    "\n",
    "# Create bag of words out of the full corpus\n",
    "corpus_bow = [corpus_dictionary.doc2bow(doc) for doc in corpus]\n",
    "\n",
    "print(colored(\"After filter: \", \"blue\"), f\"Dictionary contains a total of {len(corpus_dictionary)} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explore the top words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you can look at the top words that are used overall in our texts. You can do this by combining all of our messages into one long document, then you can see what are the top most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0ffa71d9354661b06cc311371cd0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Data to show', options=('All', 'medical_help', 'medical_products',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.interact_with_filters(utils.explore_top_tokens, haiti_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use a visualization tool such as a word cloud to plot the top words used in all of the messages overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8459e7f366444877b9d222e017a911fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Data to show', options=('All', 'medical_help', 'medical_products',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.interact_with_filters(utils.wordcloud_from_top_words, haiti_df, n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de473fc968d14d61aeca8870748f3486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Data to show', options=('All', 'medical_help', 'medical_products',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.interact_with_filters(utils.relative_words_visualization, haiti_df, n=50, show_other=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save the updated dataframe as a python pickle below. The pickle module is used for serializing and de-serializing a Python object structure. This means you can save any object from python and open it next time exactly the same as it was. If you save it as .csv for example some information about column types may be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#with open('haiti_df.pkl', 'wb') as f:\n",
    "#    pickle.dump(haiti_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citation:\n",
    "- Robert Munro. 2012. Processing short message communications in low-resource languages. [PhD dissertation, Stanford University]. Stanford Digital Repository. Retrieved from https://purl.stanford.edu/cg721hb0673"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
