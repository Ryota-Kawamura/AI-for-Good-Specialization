{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUXNQZtwEYiQ"
   },
   "source": [
    "# Biodiversity Monitoring: Design Phase (Part 2)\n",
    "In this lab you will develop a model for animal classification in the Karoo national park. More specifically, you'll fine tune a Neural Architecture Search Network ([NASNet model](https://arxiv.org/abs/1707.07012)) that was pre-trained on the [ImageNet dataset](https://www.image-net.org/index.php). \n",
    "\n",
    "\n",
    "This project contains various folders:\n",
    "\n",
    "- CameraTraps: Set of tools used by MegaDetector\n",
    "- ai4eutils: Shared utilities developed by the Microsoft AI for Earth team. Project used by MegaDetector\n",
    "- yolov5: Open-source AI project for computer vision. Used by MegaDetector\n",
    "- data: Raw images from the Karoo dataset\n",
    "- data_crops: Output images from the MegaDetector\n",
    "- models: Pretrained NASNet model with Karoo dataset\n",
    "\n",
    "The steps you will complete in this notebook are the following:\n",
    "1. Import Python packages\n",
    "2. Try out the original NASNet model\n",
    "3. Explore data augmentation \\\n",
    " 3.1 Flipping \\\n",
    " 3.2 Zooming \\\n",
    " 3.3 Rotating \\\n",
    " 3.4 Contrasting\\\n",
    " 3.5 Combining different image augmentations \n",
    "4. Balance the dataset\n",
    "5. Create your own model based on NASNet \\\n",
    " 5.1 Load the pre-trained fine-tuned NASNet model \\\n",
    " 5.2 Perform further fine-tuning of the NASNet model (optional)\n",
    "6. Evaluate your model \\\n",
    " 6.1 Confusion matrix \\\n",
    " 6.2 Visual inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E84Vw8mbvHol"
   },
   "source": [
    "## 1. Import Python packages\n",
    "\n",
    "Run the next cell to import that Python packages you'll need for this lab.\n",
    "\n",
    "Note the `import utils2` line. This line imports the functions that were specifically written for this lab. If you want to look at what these functions are, go to `File -> Open...` and open the `utils2.py` file to have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCKd1BvSvHom",
    "outputId": "a4939a59-7491-4fe9-fc2a-2adde4445e40"
   },
   "outputs": [],
   "source": [
    "import os, sys        # packages to interact with the Operating System\n",
    "import pandas as pd   # package for reading in and manipulating data\n",
    "import numpy as np    # package for numerical operations\n",
    "import matplotlib.pyplot as plt # package to create plots\n",
    "from IPython.display import Image as IPythonImage  # package to display images in Jupyter\n",
    "\n",
    "# various packages for neural network model creation and evaluation\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.metrics import sparse_top_k_categorical_accuracy \n",
    "from tensorflow.keras.applications import nasnet\n",
    "\n",
    "# Set paths for visualization_utils\n",
    "os.environ['PYTHONPATH'] += \":/home/jovyan/work/ai4eutils\"\n",
    "os.environ['PYTHONPATH'] += \":/home/jovyan/work/CameraTraps\"\n",
    "os.environ['PYTHONPATH'] += \":/home/jovyan/work/yolov5\"\n",
    "sys.path.insert(0, \"./ai4eutils\")\n",
    "sys.path.insert(0, \"./CameraTraps\")\n",
    "sys.path.insert(0, \"./yolov5\")\n",
    "\n",
    "import utils2 # utility functions defined for this lab\n",
    "\n",
    "# Configure Python to ignore Tensorflow warnings\n",
    "utils2.ignore_tf_warning_messages()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "tf.keras.utils.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "print('All packages imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Try out the original NASNet model\n",
    "\n",
    "First, let's have a look at how the original model performs. You will be using a NASNet model, which has been pre-trained on ImageNet to classify images into 1000 object categories. NASNet-Mobile is a convolutional neural network that is trained on more than a million images from the ImageNet database. The network can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals. As a result, the network has learned rich feature representations for a wide range of images. The network has an image input size of 224-by-224 pixels with 3 color channels.\n",
    "\n",
    "<img src='img/nasnet_schema.png'>\n",
    "\n",
    "Load the NASNet model using the cell below (this may take a few moments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained NASNet model\n",
    "original_nasnet_model = nasnet.NASNetMobile(include_top=True)\n",
    "print(\"NASNet model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to classify images using NASNet mobile. Try it out on a few animals and see how the model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test image prediction with the pre-trained NASNet model on the Snapshot Karoo dataset\n",
    "IMAGE_DIR = 'data_crops'\n",
    "label2cat_full = {i:category for i, category in enumerate(sorted(next(os.walk(f'{IMAGE_DIR}/train'))[1]))}\n",
    "cat2label_full = {v:k for k,v in label2cat_full.items()}\n",
    "TEST_DIR = IMAGE_DIR+'/test'\n",
    "IMAGE_SIZE = (224, 224)\n",
    "test_imgs = utils2.get_test_imgs(TEST_DIR)\n",
    "\n",
    "utils2.pick_img_and_plot_predictions(test_imgs, original_nasnet_model, nasnet.decode_predictions, cat2label_full, IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Explore data augmentation\n",
    "\n",
    "Throughout the next several cells, you'll investigate examples of image augmentation. \n",
    "\n",
    "First, run the next cell to load a single batch (32 images) of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch variables\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (224, 224)\n",
    "OUTPUT_DIR = 'data_final'\n",
    "\n",
    "# Load in the images as TensorFlow datasets\n",
    "train_ds_full, _, _ = utils2.load_data(IMAGE_DIR, BATCH_SIZE, IMAGE_SIZE, RANDOM_SEED)\n",
    "\n",
    "# Load example data\n",
    "images, labels = next(iter(train_ds_full))\n",
    "print('examples loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to select a single image. You can choose any value from 0 to 31 for the `selected_image` parameter below to look at different examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an example image for augmentation (must be a number between 0 and 31)\n",
    "selected_image = 3\n",
    "\n",
    "image = images[selected_image].numpy().astype(\"uint8\")\n",
    "label = label2cat_full[labels[selected_image].numpy()]\n",
    "\n",
    "utils2.plot_single_image(image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Flipping\n",
    "\n",
    "Look at the effect of flipping an image horizontally or vertically. You can randomly apply this transformation to a picture, to produce new images for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment the image with a flip\n",
    "utils2.data_aug_flip(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Zooming\n",
    "\n",
    "Zooming in or out at image is another way to produce new images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment the image with a zoom factor\n",
    "utils2.data_aug_zoom(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Rotating\n",
    "\n",
    "Rotating an image among its center is another way to produce new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment the image with a rotation\n",
    "utils2.data_aug_rot(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Changing contrast\n",
    "\n",
    "Or changing the contrast to an image can be considered as a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment the image with a contrast \n",
    "utils2.data_aug_contrast(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Combining different image augmentations\n",
    "Run the following cell to randomly flip, rotate, contrast, and zoom the input image.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a random set of image augmentations\n",
    "utils2.data_aug_random(image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Balance the Dataset\n",
    "\n",
    "You saw that the original NASNet model doesn't work well on your data. Now, you will use the base of the NASNet model and train a new top for predicting the classes that you have in your dataset. But before you do that, remember that you identified during data exploration that some classes have much more examples than others. So, before moving forward, you will balance the dataset using an `oversampling` and `undersampling` technique. Oversampling consists of creating several copies of the examples from the classes with fewer examples until all categories have the same number of examples. Undersampling means removing data from the classes with more entries until all categories have the same number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the data\n",
    "utils2.resample_data('data_crops', OUTPUT_DIR, train_ds_full, 11, 500)\n",
    "\n",
    "# Load in the images as TensorFlow datasets\n",
    "train_ds, val_ds, test_ds = utils2.load_data(OUTPUT_DIR, BATCH_SIZE, IMAGE_SIZE, RANDOM_SEED)\n",
    "\n",
    "# Get the labels and categories\n",
    "label2cat = {i:category for i, category in enumerate(sorted(next(os.walk(f'{OUTPUT_DIR}/train'))[1]))}\n",
    "cat2label = {v:k for k,v in label2cat.items()}\n",
    "\n",
    "label2cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to plot a bar chart of the number of example images in each class before and after resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count up the number of example images for each class of animal before and after resampling\n",
    "count_original = utils2.count_examples_per_class(train_ds_full, label2cat_full, cat2label_full)\n",
    "count_resampled = utils2.count_examples_per_class(train_ds, label2cat, cat2label)\n",
    "\n",
    "# Plot the bar charts \n",
    "utils2.plot_histograms_of_data(count_original, count_resampled, cat2label, label2cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create your own model based on NASNet\n",
    "Now, you will take only the core of the NASNet model and perform additional fine tuning on it. You'll do this by modifying only the final layers of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04oNQcWlvHos"
   },
   "outputs": [],
   "source": [
    "# Load the NASNet pre-trained base model\n",
    "base_model = nasnet.NASNetMobile(include_top=False)\n",
    "print(\"NASNet model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out a summary of the NASNet model architecture\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you must replace the top layers(output) with a fully connected layer having as many outputs as classes you want to train on. Here, you'll restrict the data to only classes which have several dozen images and remove the other classes, leaving you with 11 classes total. \n",
    "\n",
    "We also added a pre-processing layer for data augmentation between the input and the NASNet-Mobile Core. During training, the weights of the NASNet-Mobile core won't change(they are frozen). Only the weights of the newly replaced top layers will be updated. This process is called \"transfer learning.\"\n",
    "\n",
    "<img src='img/nasnet_karoo.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Load the pre-trained fine-tuned NASNet model\n",
    "\n",
    "Here we have already fine-tuned the pre-trained NASNet model for 150 epochs because fine-tuning in this lab environment would take too long. However, you'll have the option to train for additional epochs below to see what that looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXM9-40GvHos"
   },
   "outputs": [],
   "source": [
    "# Prepare a model for fine tuning\n",
    "base_model.trainable = False\n",
    "NUM_CLASSES = 11\n",
    "model = utils2.get_transfer_model(\n",
    "    model_to_transfer=base_model,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    img_height=IMAGE_SIZE[0],\n",
    "    img_width=IMAGE_SIZE[1],\n",
    ")\n",
    "\n",
    "# Load the fine tuned model weights\n",
    "model_weight_path = 'models/model_cnn_finetuned_nasnet_150epocha_augmented.h5'\n",
    "model.load_weights(model_weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Further fine-tune the NASNet model (optional)\n",
    "\n",
    "Run the next cell to train for additional epochs if you like. You can change the number of epochs to any number you like but keep in mind it takes about 7 minutes per training epoch in this lab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J399YfbmaMfP",
    "outputId": "b4bb9edb-1f5f-4c68-dcf8-291cfebe5a6d"
   },
   "outputs": [],
   "source": [
    "# Run additional fine tuning epochs\n",
    "epochs = 1\n",
    "history_finetune = model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlv__3ZDvHot"
   },
   "source": [
    "Check the training and validation accuracy and loss of the training process. Accuracy should be increasing as your model learns, and loss should decrease as your model makes fewer errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history for the fine tuning process\n",
    "utils2.plot_training_history('history_training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayFHVWUPvHou"
   },
   "source": [
    "## 6. Model evaluation\n",
    "\n",
    "Check your fine tuned model's performance on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0lKWjWusM7g"
   },
   "source": [
    "### 6.1 Confusion matrix\n",
    "With the next two cells you'll generate a confusion matrix to visualize exactly where your model is doing well and where it is having a harder time with classification. \n",
    "\n",
    "First, run the next cell to create two lists of the true classes `y_true` and the predicted classes `y_pred` for the test set of data that was not used in training or validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xe9FHUJUvHov",
    "outputId": "3806bf44-28ee-41a6-ce0e-b856ee7b8eb3"
   },
   "outputs": [],
   "source": [
    "# Create lists for storing the predictions and labels\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# Loop over the data generator\n",
    "for data, label in test_ds:\n",
    "    # Make predictions on data using the model. Store the results.\n",
    "    y_pred.extend(tf.argmax(model.predict(data), axis=1).numpy())\n",
    "    # Store corresponding labels\n",
    "    y_true.extend(label.numpy())\n",
    "# Print out the overall accuracy of the model predictions\n",
    "print('Overall model accuracy:', np.sum(np.array(y_true) == y_pred)/len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aLbD1EFmCeg"
   },
   "source": [
    "Run the next cell to visualize your confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXzLPjrBoXwC"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "utils2.plot_cm(y_true, y_pred, label2cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Kilod6cDrGN"
   },
   "source": [
    "### 6.2 Visual inspection\n",
    "\n",
    "Run the next cell to select an animal from the dropdown and see your model's predictions. The chart on the right shows your model confidence for the top three predictions, where the confidence bar appears green for the correct prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot images and the top three model predictions by confidence level\n",
    "utils2.pick_img_and_plot_predictions(test_imgs, model, label2cat, cat2label, IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Congratulations on finishing this lab!**\n",
    "\n",
    "**Keep up the good work :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
